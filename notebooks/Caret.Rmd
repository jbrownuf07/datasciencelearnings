---
title: "Caret"
output: html_notebook
---

# Visualizations

# Pre-processing

## Creating Dummy Variables

The function `dummyVars` can be used to generate complete (less than full rank parameterized) set of dummy variables from one or more factors. The function takes a formula and a data set and outputs an object that can be used to create the dummy variables using the predict method.

For example, the `etitanic` data set in the `earth` package includes 2 factors: `pclass` (passenger class, with levels 1st, 2nd, 3rd) and `sex` (with levels male and female). The base R function `model.matrix` would generate the following variables:

```{r}
library(caret)
library(earth)
data("etitanic")

head(model.matrix(survived ~ .,data = etitanic))
```

```{r}
## Now, using dummyVars
dummies <- dummyVars(survived ~ ., data = etitanic)
head(predict(dummies, newdata = etitanic))
```

Note with `dummyVars`, there is no intercept and each factor has a dummy variable for each level, so this parameterization may not be useful for some model functions, such as `lm`.

## Zero- and Near Zero-Variance Predictors

In some situations, the data generating mechanism can create predictors that only have a single unique value (ie. a "zero-variance predictor"). For many models (excluding tree-based models), this may cause the model to crash or the fit to be unstable.

Similarly, predictors might have only a handful of unique values that occur with very low frequency. For example, in the drug resistance data set, the `nR11` descriptor (number of 11-membered rings) data have a few unique numeric values that are highly unbalanced.

```{r}
data(mdrr)
data.frame(table(mdrrDescr$nR11))
```

The concern here is that these predictors may become zero-variance predictors when the data is split into cross-validation/bootstrap sub-samples or that a few samples may have an undue influence on the model. These "near-zero-variance" predictors may need to be identified and eliminated prior to modeling.

To identify these types of predictors, the following two metrics can be calculated:

* the frequency of the most prevalent value over the second most frequent value (called the "**frequency ratio**"), which would be near 1 for well-behaved predictors and highly unbalanced data

* the "percent of unique values" is the number of unique values divided by the total number of samples (times 100) that approaches 0 as the granularity of the data increases.

If the frequency ratio is greater than some predefined threshold and the percent of unique values is less than a threshold, we might consider a predictor to be near-zero-variance.

We would not want to falsely identify data that have low granularity but are evenly distributed, such as data from a uniform distribution. Therefore, using both criteria should not falsely detect such predictors.

Looking at the MDRR data, the `nearZeroVar` function can be used to identify near zero-variance variables (the `saveMetrics` argument can be used to show the details and defaults to `FALSE`):

```{r}
nzv <- nearZeroVar(mdrrDescr, saveMetrics = TRUE)
nzv # All results
nzv[nzv$nzv,] # Filtered for near-zero variance predictors
```

```{r}
dim(mdrrDescr)
```

```{r}
nzv <- nearZeroVar(mdrrDescr)
# Produces the following result:
# [1]  22  31  32  34  38  41  42 259 262 263 264 266 267 270 271 272 273 274 276 277 278 279 280 281
# [25] 282 283 284 285 286 287 288 327 328 330 331 333 334 335 336 337 338 339 340 341 342

filteredDescr <- mdrrDescr[, -nzv]
dim(filteredDescr)
```

By default, the `nearZeroVar` function will return the positions of the variables that are flagged to be problematic.

## Identifying Correlated Predictors

While there are some models that thrive on correlated predictors (such as `pls`), other models may benefit from reducing the level of correlation between predictors.

**TODO:** why does `pls` thrive on correlated predictors? I need to look into this further.

Given a correlation matrix, the `findCorrelation` function uses the following algorithm to flag predictors for removal:

```{r}
descrCor <- cor(filteredDescr) # Get the correlation matrix
highCor <- sum(abs(descrCor[upper.tri(descrCor)]) > 0.999)
```

`highCor` = 65. Therefore, there are 65 descriptors that are almost perfectly correlated (|correlation| > 0.999), such as the total information index of atomic composition (`IAC`) and the total information content index (neighborhood symmetry of 0-order)(`TICO`) (correlation = 1). The code chunk below shows the effect of removing descriptors with absolute correlations above 0.75.

```{r}
descrCor <- cor(filteredDescr)
summary(descrCor[upper.tri(descrCor)])
```

```{r}
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)
filteredDescr <- filteredDescr[,-highlyCorDescr]
descrCor2 <- cor(filteredDescr)
summary(descrCor2[upper.tri(descrCor2)])
```

## Linear Dependencies

The function `findLinearCombos` uses the QR decomposition of a matrix to enumerate sets of linear combinations (if they exist). For example, consider the following matrix that is could have been produced by a less-than-full-rank parameterizations of a two-way experimental layout:

```{r}
ltfrDesign <- matrix(0, nrow=6, ncol=6)
ltfrDesign[,1] <- c(1, 1, 1, 1, 1, 1)
ltfrDesign[,2] <- c(1, 1, 1, 0, 0, 0)
ltfrDesign[,3] <- c(0, 0, 0, 1, 1, 1)
ltfrDesign[,4] <- c(1, 0, 0, 1, 0, 0)
ltfrDesign[,5] <- c(0, 1, 0, 0, 1, 0)
ltfrDesign[,6] <- c(0, 0, 1, 0, 0, 1)
```

Note that columns two and three add up to the first column. Similarly, columns four, five, and six add up to the first column. `findLinearCombos` will return a list that enumerates these dependencies. For each linear combination, it will incrementally remove columns from the matrix and test to see if the dependencies have been resolved. `findLinearCombos` will also return a vector of column positions that can be removed to eliminate the linear dependencies:

```{r}
comboInfo <- findLinearCombos(ltfrDesign)
comboInfo
```

```{r}
ltfrDesign[,-comboInfo$remove]
```

These kinds of dependencies can arise when large number of binary chemical fingerprints are used to describe the structure of a molecule.

## The `preProcess` Function

The `preProcess` class can be used for many operations on predictions, including centering and scaling. The function `preProcess` estimates the required parameters for each operation and `predict.preProcess` is used to apply them to specific data sets. This function can also be interfaces when calling the `train` function. 

### Centering and Scaling

In the example below, half of the MDRR data is used to estimate the location and scale of the predictors. The function `preProcess` doesn't actually pre-process the data. `predict.preProcess` is used to pre-process this and other data sets.

```{r}
set.seed(96)

inTrain <- sample(seq(along = mdrrClass), length(mdrrClass) / 2)

training <- filteredDescr[inTrain, ]
test <- filteredDescr[-inTrain, ]
trainMDRR <- mdrrClass[inTrain]
testMDRR <- mdrrClass[-inTrain]

preProcValues <- preProcess(training, method = c('center', 'scale'))

trainTransformed <- predict(preProcValues, training)
testTransformed <- predict(preProcValues, test)
```

The `preProcess` option `"range"` scales the data to the interval between 0 and 1.

### Imputation

`preProcess` can be used to impute data sets based only on information in the training set. One method of doing this is with K-nearest neighbors. For an arbitrary sample, the K closest neighbors are found in the training set and the value for the predictor is imputed using these values (*e.g.* using the mean). Using this approach will automatically trigger `preProcess` to center and scale the data, regardless of what is in the `method` argument. Alternatively, bagged trees can also be used to impute. For each predictor in the data, a bagged tree is created using all of the other predictors in the training set. When a new sample has a missing predictor value, the bagged model is use to predict the value. While, in theory, this is a more powerful method of imputing, the computational costs are much higher than the nearest neighbor technique.

### Transforming Predictors

In some cases, there is a need to use Principal Components Analysis (PCA) to transform the data to a smaller sub-space where the new variables are uncorrelated with one another. The `preProcess` class can apply this transformation by including `"pca"` in the `method` argument. Doing this will also for scaling of the predictors. Note that when PCA is requested, `predict.preProcess` changes the column names to `PC1`, `PC2`, and so on. 

Similarly, independent component analysis (ICA) can also be used to find new variables that are linear combinations of the original set such that the components are independent (as opposed to uncorrelated with PCA). The new variables will be labeled as `IC1`, `IC2`, and so on.

The "spatial sign" transformation projects the data for a predictor to the unit circle in p dimensions, where p is the number of predictors. Essentially, a vector of data is divided by its norm. The two figures below show two centered and scaled dimensions from the MDRR data set before and after the spatial sign transformation. The predictors should be centered and scaled before applying this transformation.

```{r}
library(AppliedPredictiveModeling)
transparentTheme(trans = 0.4)

plotSubset <- data.frame(scale(mdrrDescr[, c('nC', 'X4v')]))
xyplot(nC ~ X4v,
       data = plotSubset,
       groups = mdrrClass,
       auto.key = list(columns = 2))
```

After the spatial transformation:

```{r}
transformed <- spatialSign(plotSubset)
transformed <- as.data.frame(transformed)
xyplot(nC ~ X4v,
       data = transformed,
       groups = mdrrClass,
       auto.key = list(columns = 2))
```

Another option `"BoxCox"` will estimate a Box-Cox tranformation on the predictors **if the data are greater than zero**.

```{r}
preProcessValues2 <- preProcess(training, method = 'BoxCox')
trainBC <- predict(preProcessValues2, training)
testBC <- predict(preProcessValues2, test)
preProcessValues2
```

The `NA` values correspond to the predictors that could not be transformed. This transformation requires the data to be greater than zero. Two similar transformations, the Yeo-Johnson and exponential transformation of Manly (1976) can also be used in `preProcess`.

### Putting it all together

In *Applied Predictive Modeling* there is a case study where the execution times of jobs in high performance computing environments are being predicted. The data are:

```{r}
library(AppliedPredictiveModeling)
data("schedulingData")
str(schedulingData)
```

The data are a mix of categorical and numeric predictors. Suppose we want to use the Yeo-Johnson transformation on the continuous predictors then center and scale them. Let's also suppose we will be running a tree-based model so we might want to keep the factors as factors (as opposed to creating dummy variables). We run the function on all columns except the last, which is the outcome.

```{r}
pp_hpc <- preProcess(schedulingData[, -8],
                     method = c('center', 'scale', 'YeoJohnson'))
pp_hpc
```

```{r}
transformed <- predict(pp_hpc, newdata = schedulingData[, -8])
head(transformed)
```


# Data Splitting

# Model Training and Tuning

# Available Models

# `train` Models by Tag

# Models Clustered by Tag Similarity

# Parallel Processing

# Random Hyperparameter Search

# Subsampling For Class Imbalances

# Using Recipes with `train`

# Using Your Own Model in `train`

# Adaptive Resampling

# Variable Importance

# Miscellaneous Model Functions

# Measuring Performance

# Feature Selection Overview

# Feature Selection using Univariate Filters

# Recursive Feature Elimination

# Feature Selection using Genetic Algorithms

# Feature Selection using Simulated Annealing

# Data Sets


