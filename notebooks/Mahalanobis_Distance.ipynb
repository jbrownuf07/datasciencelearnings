{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mahalanobis Distance\n",
    "\n",
    "[Reference](https://www.machinelearningplus.com/statistics/mahalanobis-distance/)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Mahalanobis distance is an effective multivariate distance metric that measures the distance between a point (vector) and a distribution.\n",
    "\n",
    "It has excellent applications in multivariate anomaly detection, classification on highly imbalanced datasets, and one-class classification and more untapped use cases.\n",
    "\n",
    "Considering its extremely useful applications, this metric is seldom discussed or used in stats or ML workflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's wrong with using Euclidean Distance for Multivariate data?\n",
    "\n",
    "Let's start with the basics. Euclidean distance is the commonly used straight line distance between two points.\n",
    "\n",
    "If the two points are in a two-dimensional plane (meaning, you have two numeric columns (p) and (q)) in your dataset), then the Euclidean distance between the two points (p1, q1) and (p2, q2) is:\n",
    "\n",
    "\\begin{equation}\n",
    "d(p,q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}\n",
    "\\end{equation}\n",
    "\n",
    "This formula may be extended to as many dimensions as you wish:\n",
    "\n",
    "\\begin{equation}\n",
    "d(p,q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \\ldots + (p_n - q_n)^2 }\n",
    "\\end{equation}\n",
    "\n",
    "Well, **Euclidean distance will work fine as long as the dimensions are equally weighted and are independent of each other**.\n",
    "\n",
    "What do I mean by that?\n",
    "\n",
    "Let's consider the following tables:\n",
    "\n",
    "| Area (sq.ft) | Price (thousands) | Area (acre) | Price (millions) |\n",
    "|--------------|-----------------|-------------|----------------|\n",
    "| 2400 | 156000 | 0.0550944 | 156 |\n",
    "| 1950 | 126750 | 0.0447642 | 126.75 |\n",
    "| 2100 | 105000 | 0.0482076 | 105 |\n",
    "| 1200 | 78000 | 0.0275472 | 78 |\n",
    "| 2000 | 130000 | 0.045912 | 130 |\n",
    "| 900 | 54000 | 0.0206604 | 54 |\n",
    "\n",
    "The two tables above show the 'area' and 'price' of the same objects. Only the units of the variables change.\n",
    "\n",
    "Since both tables represent the same entities, the distance between any two rows, point A and point B should be the same. But, Euclidean distance gives a different value even though the distances are technically the same in physical space.\n",
    "\n",
    "This can technically be overcome by scaling the variables, by computing the z-score (ex: (x - mean) / std) or make it vary within a particular range like between 0 and 1.\n",
    "\n",
    "But, there is another major drawback.\n",
    "\n",
    "That is, if the dimensions (columns in your dataset) are correlated to one another, which is typically the case in real-world datasets, the Euclidean distance between a point and the center of the points (distribution) can give little or misleading information about how close a point really is to the cluster.\n",
    "\n",
    "![img](img/Mahalanobis_Distance_Usecase.webp)\n",
    "\n",
    "The above image (on the right) is a simple scatterplot of two variables that are positively correlated with each other. That is, as the value of one variable (x-axis) increases, so does the value of the other variable (y-axis).\n",
    "\n",
    "The two points above are equally distant (Euclidean) from the center. But only one of them (blue) is actually ***closer*** to the cluster, even though, technically the Euclidean distance between the two points are equal.\n",
    "\n",
    "This is because Euclidean distance is a distance between two points only. It does not consider how the rest of the points in the dataset vary. So, it cannot be used to judge how close a point actually is to a distribution of points.\n",
    "\n",
    "What we need here is a more robust distance metric that is an accurate representation of how distant a point is from a *distribution*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Mahalanobis Distance?\n",
    "\n",
    "Mahalanobis distance is the distance between a point and a distribution. It is effectively a multivariate equivalent of the Euclidean distance.\n",
    "\n",
    "It was introduced by Prof. P.C. Mahalanobis in 1936 and has been used in various statistical applications ever since. However, it's not so well known or used in machine learning practice. \n",
    "\n",
    "So, computationally, how is Mahalanobis distance different from Euclidean distance?\n",
    "\n",
    "1. It transforms the columns into uncorrelated variables.\n",
    "1. Scale the columns to make their variance equal to 1\n",
    "1. Finally, it calculates the Euclidean distance.\n",
    "\n",
    "The above three steps are meant to address the problems with Euclidean distance we just talked about. But how?\n",
    "\n",
    "Let's look at the formula and try to understand its components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The math and intuition behind Mahalanobis Distance\n",
    "\n",
    "The formula to compute Mahalanobis distance is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "D^2 = (x - m)^T \\cdot C^{-1} \\cdot (x - m)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where, \n",
    "- **D^2** is the square of the Mahalanobis distance\n",
    "- **x** is the vector of the observation (row in a dataset)\n",
    "- **m** is the vector of mean values of independent variables (mean of each column)\n",
    "- **C^(-1)** is the inverse covariance matrix of independent variables.\n",
    "\n",
    "So, how to understand the above equation?\n",
    "\n",
    "Let's take the $(x-m)^T \\cdot C^{-1}$ term.\n",
    "\n",
    "$(x-m)$ is essentially the distance of the vector from the mean. We then divide this by the covariance matrix (or multiply by the inverse of the covariance matrix).\n",
    "\n",
    "If you think about it, this is essentially a multivariate equivalent of the regular standardization ($z = (x - \\mu)/\\sigma$). That is, z = (x vector) - (mean vector) / (covariance matrix).\n",
    "\n",
    "So, what is effect of dividing by the covariance?\n",
    "\n",
    "If the variables in your dataset are strongly correlated, then, the covariance will be high. Dividing by a large covariance will effectively reduce the distance.\n",
    "\n",
    "Likewise, if the X's are not correlated, then the covariance is not high and the distance is not reduced much.\n",
    "\n",
    "So effectively, it addresses both the problems of scale as well as the correlation of the variables that we talked about in the introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compute Mahalanobis Distance in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>61.5</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>59.8</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>56.9</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.29</td>\n",
       "      <td>62.4</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.31</td>\n",
       "      <td>63.3</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat  depth  price\n",
       "0   0.23   61.5    326\n",
       "1   0.21   59.8    326\n",
       "2   0.23   56.9    327\n",
       "3   0.29   62.4    334\n",
       "4   0.31   63.3    335"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = 'data/diamonds.csv'\n",
    "df = pd.read_csv(filepath).iloc[:, [0,4,6]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the function to calculate Mahalanobis distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>price</th>\n",
       "      <th>mahala</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>61.5</td>\n",
       "      <td>326</td>\n",
       "      <td>1.709860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>59.8</td>\n",
       "      <td>326</td>\n",
       "      <td>3.540097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>56.9</td>\n",
       "      <td>327</td>\n",
       "      <td>12.715021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.29</td>\n",
       "      <td>62.4</td>\n",
       "      <td>334</td>\n",
       "      <td>1.454469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.31</td>\n",
       "      <td>63.3</td>\n",
       "      <td>335</td>\n",
       "      <td>2.347239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat  depth  price     mahala\n",
       "0   0.23   61.5    326   1.709860\n",
       "1   0.21   59.8    326   3.540097\n",
       "2   0.23   56.9    327  12.715021\n",
       "3   0.29   62.4    334   1.454469\n",
       "4   0.31   63.3    335   2.347239"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mahalanobis(x=None, data=None, cov=None):\n",
    "    \"\"\"Compute the Mahalanobis Distance between each row of x and the data\n",
    "    x: vector or matrix of data with, say, p columns\n",
    "    data: ndarray of the distribution from which Mahalanobis distance of each observation of x is to be computed.\n",
    "    cov: covariance matrix (p x p) of the distribution. If None, will computed from data\n",
    "    \"\"\"\n",
    "    x_minus_mu = x - np.mean(data)\n",
    "    if not cov:\n",
    "        cov = np.cov(data.values.T)\n",
    "    inv_covmat = np.linalg.inv(cov)\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    return mahal.diagonal()\n",
    "\n",
    "df_x = df[['carat', 'depth', 'price']].head(500)\n",
    "df_x['mahala'] = mahalanobis(x=df_x, data=df[['carat', 'depth', 'price']])\n",
    "df_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usecase 1: Multivariate outlier detection using Mahalanobis distance\n",
    "\n",
    "Assuming that the test statistic follows chi-square with 'n' degree of freedom, the critical value at a 0.01 significance level and 2 degrees of freedom is computed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.21034037197618"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Critical values for two degrees of freedom\n",
    "from scipy.stats import chi2\n",
    "\n",
    "chi2.ppf((1 - 0.01), df = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that an observation can be considered extreme (outlier) if its Mahalanobis distance exceeds **9.21**.\n",
    "\n",
    "If you prefer p-values instead to determine if an observation is extreme or not, the p-values can be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>price</th>\n",
       "      <th>mahala</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>56.9</td>\n",
       "      <td>327</td>\n",
       "      <td>12.715021</td>\n",
       "      <td>0.001734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>55.1</td>\n",
       "      <td>2757</td>\n",
       "      <td>23.909643</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>66.3</td>\n",
       "      <td>2759</td>\n",
       "      <td>11.781773</td>\n",
       "      <td>0.002765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>1.17</td>\n",
       "      <td>60.2</td>\n",
       "      <td>2774</td>\n",
       "      <td>9.279459</td>\n",
       "      <td>0.009660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.98</td>\n",
       "      <td>67.9</td>\n",
       "      <td>2777</td>\n",
       "      <td>20.086616</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.70</td>\n",
       "      <td>57.2</td>\n",
       "      <td>2782</td>\n",
       "      <td>10.405659</td>\n",
       "      <td>0.005501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.84</td>\n",
       "      <td>55.1</td>\n",
       "      <td>2782</td>\n",
       "      <td>23.548379</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>1.05</td>\n",
       "      <td>65.8</td>\n",
       "      <td>2789</td>\n",
       "      <td>11.237146</td>\n",
       "      <td>0.003630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>1.00</td>\n",
       "      <td>58.2</td>\n",
       "      <td>2795</td>\n",
       "      <td>10.349019</td>\n",
       "      <td>0.005659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>1.01</td>\n",
       "      <td>67.4</td>\n",
       "      <td>2797</td>\n",
       "      <td>17.716144</td>\n",
       "      <td>0.000142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     carat  depth  price     mahala   p_value\n",
       "2     0.23   56.9    327  12.715021  0.001734\n",
       "91    0.86   55.1   2757  23.909643  0.000006\n",
       "97    0.96   66.3   2759  11.781773  0.002765\n",
       "172   1.17   60.2   2774   9.279459  0.009660\n",
       "204   0.98   67.9   2777  20.086616  0.000043\n",
       "221   0.70   57.2   2782  10.405659  0.005501\n",
       "227   0.84   55.1   2782  23.548379  0.000008\n",
       "255   1.05   65.8   2789  11.237146  0.003630\n",
       "284   1.00   58.2   2795  10.349019  0.005659\n",
       "298   1.01   67.4   2797  17.716144  0.000142"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the p-values\n",
    "df_x['p_value'] = 1 - chi2.cdf(df_x['mahala'], 2)\n",
    "\n",
    "# Extreme values with a significance level of 0.01\n",
    "df_x.loc[df_x.p_value < 0.01].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare the above observations against the rest of the datasert, they are clearly extreme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usecase 2: Mahalanobis distance for Classification Problems\n",
    "\n",
    "Mahalanobis distance can be used for classification problems. A naive implementation of a Mahalanobis classifier is coded below. The intuition is that, an observation is assigned the class that is is closest to based on the Mahalanobis distance.\n",
    "\n",
    "Let's see an example implemented on the `BreastCancer` dataset, where the objective is to determine if a tumour . is benign or malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cl.thickness</th>\n",
       "      <th>Cell.size</th>\n",
       "      <th>Marg.adhesion</th>\n",
       "      <th>Epith.c.size</th>\n",
       "      <th>Bare.nuclei</th>\n",
       "      <th>Bl.cromatin</th>\n",
       "      <th>Normal.nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cl.thickness  Cell.size  Marg.adhesion  Epith.c.size  Bare.nuclei  \\\n",
       "0             5          1              1             2          1.0   \n",
       "1             5          4              5             7         10.0   \n",
       "2             3          1              1             2          2.0   \n",
       "3             6          8              1             3          4.0   \n",
       "4             4          1              3             2          1.0   \n",
       "\n",
       "   Bl.cromatin  Normal.nucleoli  Mitoses  Class  \n",
       "0            3                1        1      0  \n",
       "1            3                2        1      0  \n",
       "2            3                1        1      0  \n",
       "3            3                7        1      0  \n",
       "4            3                1        1      0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_filepath = 'data/BreastCancer.csv'\n",
    "columns_to_use = ['Cl.thickness', 'Cell.size', 'Marg.adhesion', \n",
    "                          'Epith.c.size', 'Bare.nuclei', 'Bl.cromatin', 'Normal.nucleoli', \n",
    "                          'Mitoses', 'Class']\n",
    "df = pd.read_csv(bc_filepath, usecols = columns_to_use)\n",
    "\n",
    "df.dropna(inplace=True) # drop observations with missing data\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset in 70:30 ratio as Train and Test. And the training dataset is split into homogeneous groups of 'pos'(1) and 'neg'(0) classes. To predict the class of the test dataset, we measure the Mahalanobis distances between a given observation (row) and both the positive (`xtrain_pos`) and negative (`xtrain_neg`).\n",
    "\n",
    "Then, that observation is assigned the class based on the group it is closest to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(df.drop('Class', axis = 1), df['Class'], test_size = 0.3, random_state = 100)\n",
    "\n",
    "# Split the training data as pos and neg\n",
    "xtrain_pos = xtrain.loc[ytrain == 1, :]\n",
    "xtrain_neg = xtrain.loc[ytrain == 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the `MahalanobisBinaryClassifier`. To do that, you need to define the `predict_proba()` and the `predict()` methods. This classifier does not require a separate `fit()` (training) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pred  true\n",
      "0     0     0\n",
      "1     1     1\n",
      "2     0     0\n",
      "3     0     0\n",
      "4     0     0\n"
     ]
    }
   ],
   "source": [
    "class MahalanobisBinaryClassifier():\n",
    "    def __init__(self, xtrain, ytrain):\n",
    "        self.xtrain_pos = xtrain.loc[ytrain == 1, :]\n",
    "        self.xtrain_neg = xtrain.loc[ytrain == 0, :]\n",
    "        \n",
    "    def predict_proba(self, xtest):\n",
    "        pos_neg_dists = [(p,n) for p, n in zip(mahalanobis(xtest, self.xtrain_pos), mahalanobis(xtest, self.xtrain_neg))]\n",
    "        return np.array([(1 - n/(p+n), 1 - p/(p+n)) for p, n in pos_neg_dists])\n",
    "    \n",
    "    def predict(self, xtest):\n",
    "        return np.array([np.argmax(row) for row in self.predict_proba(xtest)])\n",
    "    \n",
    "clf = MahalanobisBinaryClassifier(xtrain, ytrain)\n",
    "pred_probs = clf.predict_proba(xtest)\n",
    "pred_class = clf.predict(xtest)\n",
    "\n",
    "# Pred and Truth\n",
    "pred_actuals = pd.DataFrame([(pred, act) for pred, act in zip(pred_class, ytest)], columns=['pred', 'true'])\n",
    "print(pred_actuals[:5])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the classifier performed on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC:  0.990974358974359\n",
      "\n",
      "Confusion Matrix: \n",
      " [[113  17]\n",
      " [  0  75]]\n",
      "\n",
      "Accuracy Score:  0.9170731707317074\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93       130\n",
      "           1       0.82      1.00      0.90        75\n",
      "\n",
      "    accuracy                           0.92       205\n",
      "   macro avg       0.91      0.93      0.91       205\n",
      "weighted avg       0.93      0.92      0.92       205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "truth = pred_actuals.loc[:, 'true']\n",
    "pred = pred_actuals.loc[:, 'pred']\n",
    "scores = np.array(pred_probs)[:, 1]\n",
    "\n",
    "print('AUROC: ', roc_auc_score(truth, scores))\n",
    "print('\\nConfusion Matrix: \\n', confusion_matrix(truth, pred))\n",
    "print('\\nAccuracy Score: ', accuracy_score(truth, pred))\n",
    "print('\\nClassification Report: \\n', classification_report(truth, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalanobis distance alone is able to contribute to this much accuracy (92%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usecase 3: One-class Classification\n",
    "\n",
    "One Class classification is a type of algorithm where the training dataset contains observations belonging to only one class.\n",
    "\n",
    "With only that information known, the objective is to figure out if a given observation in a new (or test) dataset belongs to that class.\n",
    "\n",
    "You might wonder when would such a situation occur. Well, it's a quite common problem in Data Science.\n",
    "\n",
    "For example, consider the following situation: You have a large dataset containing millions of records that are NOT yet categorized as 1's or 0's. But, you also have with you a small sample dataset containing only positive (1's) records. By learning the information in this sample dataset, you want to classify all the records in the large dataset as 1's and 0's.\n",
    "\n",
    "Based on the information from the dataset, it is possible to tell if any given sample is a 1 or 0 by viewing only the 1's (and having no knowledge of the 0's at all).\n",
    "\n",
    "This can be done using Mahalanobis Distance.\n",
    "\n",
    "Let's try this on the `BreastCancer` dataset, only this time we will consider only the malignant observations (class column=1) in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(bc_filepath, usecols = columns_to_use)\n",
    "\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting 50% of the dataset into training and test. Only the 1's are retained in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(df.drop('Class', axis=1), df['Class'], test_size = 0.5, random_state=100)\n",
    "\n",
    "# Split the training data as pos and neg\n",
    "xtrain_pos = xtrain.loc[ytrain == 1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the `MahalanobisOneClassClassifier` and get the Mahalanobis Distance of each datapoint in x from the training set (`xtrain_pos`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical value is 14.067140449340169\n",
      "       mahal  true_class\n",
      "0  13.104716           0\n",
      "1  14.408570           1\n",
      "2  14.932236           0\n",
      "3  14.588622           0\n",
      "4  15.471064           0\n"
     ]
    }
   ],
   "source": [
    "class MahalanobisOneClassClassifier():\n",
    "    def __init__(self, xtrain, significance_level=0.01):\n",
    "        self.xtrain = xtrain\n",
    "        self.critical_value = chi2.ppf((1 - significance_level), df = xtrain.shape[1] - 1)\n",
    "        print(f'Critical value is {self.critical_value}')\n",
    "        \n",
    "    def predict_proba(self, xtest):\n",
    "        mahalanobis_dist = mahalanobis(xtest, self.xtrain)\n",
    "        self.pvalues = 1 - chi2.cdf(mahalanobis_dist, 2)\n",
    "        return mahalanobis_dist\n",
    "    \n",
    "    def predict(self, xtest):\n",
    "        return np.array([int(i) for i in self.predict_proba(xtest) > self.critical_value])\n",
    "        \n",
    "clf = MahalanobisOneClassClassifier(xtrain_pos, significance_level=0.05)\n",
    "mahalanobis_dist = clf.predict_proba(xtest)\n",
    "\n",
    "# Pred and Truth\n",
    "mdist_actuals = pd.DataFrame([(m, act) for m, act in zip(mahalanobis_dist, ytest)], columns=['mahal', 'true_class'])\n",
    "print(mdist_actuals[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the Mahalanobis distance and the actual class of each observation.\n",
    "\n",
    "I would expect those observations with low Mahalanobis distance to be 1's.\n",
    "\n",
    "So, I sort the `mdist_actuals` by Mahalanobis distance and quantile cut the rows into 10 equal sized groups. The observations in the top quantiles should have more 1's compared to the ones in the bottom. Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          avg_mahaldist  sum_of_trueclass\n",
      "quantile                                 \n",
      "1              3.765496                33\n",
      "2              6.511026                32\n",
      "3              9.272944                30\n",
      "4             12.209504                20\n",
      "5             14.455050                 4\n",
      "6             15.684493                 4\n",
      "7             17.368633                 3\n",
      "8             18.840714                 1\n",
      "9             21.533159                 2\n",
      "10            23.524055                 1\n"
     ]
    }
   ],
   "source": [
    "# Quantile cut into 10 pieces\n",
    "mdist_actuals['quantile'] = pd.qcut(mdist_actuals['mahal'],\n",
    "                                   q = [0, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 1.0],\n",
    "                                   labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# Sort by Mahalanobis Distance\n",
    "mdist_actuals.sort_values('mahal', inplace = True)\n",
    "\n",
    "perc_truths = mdist_actuals.groupby('quantile').agg({'mahal': np.mean, 'true_class': np.sum}).rename(columns={'mahal':'avg_mahaldist', 'true_class':'sum_of_trueclass'})\n",
    "print(perc_truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you notice above, nearly 90% of the 1's (malignant cases) fall within the first 40% percentile of the Mahalanobis distance. Incidently, all of these are lower than the critical value of 14.05. So, let's set the critical value as the cutoff and mark those observations with Mahalanobis distance less than the cutoff as positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix: \n",
      " [[183  29]\n",
      " [ 15 115]]\n",
      "\n",
      "Accuracy Score:  0.8713450292397661\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89       212\n",
      "           1       0.80      0.88      0.84       130\n",
      "\n",
      "    accuracy                           0.87       342\n",
      "   macro avg       0.86      0.87      0.87       342\n",
      "weighted avg       0.88      0.87      0.87       342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Positive if mahalanobis \n",
    "pred_actuals = pd.DataFrame([(int(p), y) for y, p in zip(ytest, clf.predict_proba(xtest) < clf.critical_value)], columns=['pred', 'true'])\n",
    "\n",
    "# Accuracy Metrics\n",
    "truth = pred_actuals.loc[:, 'true']\n",
    "pred = pred_actuals.loc[:, 'pred']\n",
    "print('\\nConfusion Matrix: \\n', confusion_matrix(truth, pred))\n",
    "print('\\nAccuracy Score: ', accuracy_score(truth, pred))\n",
    "print('\\nClassification Report: \\n', classification_report(truth, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, without the knowledge of the benign class, we are able to accurately predict the class of 87% of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
